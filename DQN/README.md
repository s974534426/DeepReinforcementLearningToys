# DQN

## 1. 算法思路

DQN将RL和DNN结合。强化学习由于连续的观测存在相关性、优化函数$Q$和目标$r+\gamma max_{a'}Q(s', a')$存在相关性，所以常常是不稳定的甚至难以收敛的。DQN中采用experience replay和target net的方式解决这种不稳定性和不收敛的问题。

图像预处理会将取当前图像和前一帧的最大值作为当前输入，然后取Y通道的值，rescale到84*84，将最近m=4帧的图像进行stack作为当前输入。

Q网络使用当前状态作为输入，输出每个行动的值(这与Q函数定义有一些不同，Q函数是要求输入状态和某个行动，返回Q值，但这种网络需要对每个行动进行一次前向传播过程，效率比较低，如果使用状态作为输入，每个行动的值作为输出，则只需要一次前向传播)。
